{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to extract quite a lot of interesting, structured information from text data simply by using string processing techiques. \n",
    "\n",
    "In this session, we'll see how to do some of these things, specifically calculating word frequencies and showing key-words-in-context (concordances). We'll do this for individual files and then you'll work together to write Python code which does this for a larger corpus of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
      "\u001b[1;31m                  [subcommand]\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter: Interactive Computing\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mpositional arguments:\n",
      "\u001b[1;31m  subcommand     the subcommand to launch\n",
      "\u001b[1;31m\n",
      "\u001b[1;31moptions:\n",
      "\u001b[1;31m  -h, --help     show this help message and exit\n",
      "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
      "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
      "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
      "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
      "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "\u001b[1;31m                 format.\n",
      "\u001b[1;31m  --json         output paths as machine-readable json\n",
      "\u001b[1;31m  --debug        output debug information about paths\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mAvailable subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os # operating system tools\n",
    "import re # regex\n",
    "import string # string processing tools\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading text files__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a filepath using ```os.path.join()``` like we saw last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join (\"..\",\"data\",\"Dickens_Expectations_1861.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to load the file that we want to work with.\n",
    "\n",
    "There are a number of ways to do this in Python, but the following should be considered \"best practice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context manager, want to open a file in a certain way (read, append, edit). We want to open in read mode\n",
    "with open(filename, \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load the text file, we just have a simple string object which can be indexed and sliced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:300])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are some formatting things that are a little funky, such as lots of newline breaks.\n",
    "\n",
    "We can get rid of those by using the ```.replace()``` method on strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:300] # get it with the formatting of the text. Unicode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize text__\n",
    "\n",
    "So far, we have one long string of characters. But we want to be able to work with individual words. To do that, we have to *tokenize* our data - in other words, to split it into individual tokens (or words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get sentences with regex__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar logic to split the data into separate sentences.\n",
    "\n",
    "This time we use a bit of ```regex``` to do our string splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(r\"[.?!]\\s*\",text) # [.?!] means [\\.\\?] the actual character \\s new line, space or tab, * matches preceding element zero or more times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find word frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count how many times an individual word appears manually, simply by iterating over the list of tokens and using a counter. \n",
    "\n",
    "To do this, we use a built in Python function called ```enumerate()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "keyword = \"love\"\n",
    "# for every token\n",
    "for token in tokens: \n",
    "    # Tokens remove punctuation at the end\n",
    "    stripped = token.strip(string.punctuation) \n",
    "    # make lowercase\n",
    "    lowered = stripped.lower() \n",
    "    # is that token the keyword?\n",
    "    if token == keyword:\n",
    "        # if yes, add 1 to the counter\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.count(\"love\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned= []\n",
    "# Tokens remove punctuation at the end\n",
    "    stripped = token.strip(string.punctuation) \n",
    "    # make lowercase\n",
    "    lowered = stripped.lower()\n",
    "    # append the vleaned to a list\n",
    "    cleaned.append(lowered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a DICTIONARY\n",
    "Counter(cleaned)\n",
    "#base them on frequency, function verbs, determiners, TUPLES \n",
    "Counter(cleaned).most_common()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar logic to find all sentences where a certain keyword appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    # make everything lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # strip sentence for punctuation\n",
    "    stripped = sentence.strip(string.punctuation)\n",
    "    # add whitespace around keyword\n",
    "    modified_kw= \" \" + keyword + \" \"\n",
    "    if modified_kw in stripped\n",
    "        print(stripped)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also has some built-in tools which we can use to count how many times a token appears in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using enumarate, allows you to go through sentences one by one, and make an index for every index (and list which linenumber it is)\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # make everything lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # strip sentence for punctuation\n",
    "    stripped = sentence.strip(string.punctuation)\n",
    "    # add whitespace around keyword\n",
    "    modified_kw= \" \" + keyword + \" \"\n",
    "    if modified_kw in stripped\n",
    "        print(idx, stripped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems, though! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing keywords in context (KWIC, concordancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned= []\n",
    "for token in tokens:\n",
    "    # remove punctuation at the end\n",
    "    stripped = token.strip(string.punctuation) \n",
    "    # make lowercase\n",
    "    lowered = stripped.lower()\n",
    "    # append the vleaned to a list\n",
    "    cleaned.append(lowered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
      "\u001b[1;31m                  [subcommand]\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter: Interactive Computing\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mpositional arguments:\n",
      "\u001b[1;31m  subcommand     the subcommand to launch\n",
      "\u001b[1;31m\n",
      "\u001b[1;31moptions:\n",
      "\u001b[1;31m  -h, --help     show this help message and exit\n",
      "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
      "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
      "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
      "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
      "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "\u001b[1;31m                 format.\n",
      "\u001b[1;31m  --json         output paths as machine-readable json\n",
      "\u001b[1;31m  --debug        output debug information about paths\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mAvailable subcommands: kernel kernelspec migrate run troubleshoot\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#define keyword\n",
    "keyword = love\n",
    "\n",
    "# for every token, going tthrough the cleaned list\n",
    "for idx, token in enumerate(cleaned):\n",
    "    #if the token is love\n",
    "    if token == keyword:\n",
    "        #from the list called cleaned, we want to take the five previous words in the list called clean\n",
    "        #.join: joining the list into a single string with 5 words before and all the words (max 5) \n",
    "        # after the keyword with a space between them\n",
    "        before= ' '.join(cleaned[idx-5:idx])\n",
    "        after = ' '.join(cleaned[idx+1:idx+6])\n",
    "        full = before [before, token, after]\n",
    "        # how many characters do I want between the surrounding words and the keyword (makes spaces until there are 50 character)\n",
    "        print(\"{:50} {:20} {:50}\".format(*full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = [\"dog\", \"cat\", \"bird\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In groups, work on the following exercises in class. \n",
    "\n",
    "I've left these somewhat underspecified, so you're welcome to solve them in whatever way you please, and to save the results in whatever format you think works best.\n",
    "\n",
    "- Write some code which searches through *all* of the novels in the folder called *100 English Novels* and shows how many times a given keyword appears in each novel.\n",
    "   - Save your results in a way which \n",
    "- Turn the KWIC in context code above into a function which can be used to show *all* occurrences of a keyword in the corpus. \n",
    "  - Bonus: Your results should show the same results as those above but with an additional column showing the filename\n",
    "  - Bonus: Write your function in such a way that a user can define the context window size to display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
